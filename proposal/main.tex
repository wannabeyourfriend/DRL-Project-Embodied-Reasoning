\documentclass[10pt,twocolumn]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

% Page length commands
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\setlength{\textwidth}{7in}
\setlength{\textheight}{9.5in}  % 增加文本区域高度
\setlength{\columnsep}{0.3125in}
\setlength{\topmargin}{-0.75in} % 减小上边距
\setlength{\headheight}{0in}    % 移除页眉空间
\setlength{\headsep}{0in}       % 移除页眉分隔
\setlength{\parindent}{0em}     % 取消段落缩进
\setlength{\parskip}{0.5ex}     % 段落间距
\pagestyle{empty}               % 无页眉页脚


\cvprfinalcopy
\usepackage[style=ieee]{biblatex}  % 引用风格
\addbibresource{egbib.bib}    % 指定bib文件
\input{main}
\begin{document}

\title{\bf Project Proposal}
\author{Contact \\wang-zx23@mails.tsinghua.edu.cn}
\date{\today}

\maketitle\input{main}
\thispagestyle{empty}

\section*{Team Information}
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
    \item \textbf{Track:} 
    \item \textbf{Team Members:} 
    \begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
        \item Zixuan Wang 2023011307
        \item Member 2 
        \item Member 3 
        \item Member 4 
        \item Member 5
    \end{itemize}
\end{itemize}

\section*{Idea Justification}
blablabla

\section*{Problem Formulation}
blablabla

\section*{Literature Survey}

\begin{enumerate}[leftmargin=*,noitemsep,topsep=0pt]
    \item \textbf{Embodied-Reasoner} (Zhang et al.) \cite{embodied-reasoner}: Develops a three-stage training pipeline (imitation learning, rejection sampling, and reflection tuning) that significantly outperforms baselines by reducing repetitive searches and improving logical consistency in embodied tasks.
    \item \textbf{ASKTOACT} (Ramrakhya et al.) \cite{ramrakhya2025groundingmultimodalllmsembodied}: Proposes an RL framework that trains multimodal LLMs to resolve instruction ambiguity through minimal clarification questions, using LLM-generated rewards to eliminate manual reward engineering.
    \item \textbf{Embodied-R} (Zhao et al.) \cite{zhao2025embodiedr}: Introduces a collaborative framework where large VLMs handle perception while small LMs perform reasoning, trained with RL using logical consistency rewards for spatial reasoning tasks.
    
    
\end{enumerate}
\textbf{Pipeline Details} 
Embodied-Reasoner implements a comprehensive three-stage pipeline: initial imitation learning on synthetic trajectories, followed by self-exploration through high-temperature sampling with trajectory selection by a learned reward model, and finally reflection tuning where the model learns to identify and correct its own mistakes. Their training data consists of 9,300 synthesized trajectories containing 64,000 interactive images paired with 90,000 reasoning steps, supplemented by human-annotated test cases.
ASK-TO-ACT firstly adapts a multimodal LLM architecture to process sequential visual observations through a Perceiver-based token downsampling method, then employ online RL training where the reward function is automatically generated by another LLM evaluating task progress and question quality. The training data comes from simulated home environments where agents interact with ambiguous object retrieval scenarios. Embodied-R takes a different approach by separating the perception and reasoning components - using a pretrained vision-language model for processing visual inputs while training a smaller language model with RL, where the key innovation is a logical consistency reward that aligns the model's reasoning process with its final answers. They train this system on a relatively small dataset of 5,000 embodied video samples with keyframe extraction to manage computational costs. 

\textbf{Reward Design and Training Methodology}
The ASKTOACT framework uses online reinforcement learning to fine-tune multimodal language models, utilizing LLM-generated sub-goals and question sequences as reward signals. Embodied-R enhances this approach by incorporating logical consistency rewards that evaluate the alignment between reasoning processes and final answers, enabling smaller language models to achieve sophisticated spatial reasoning capabilities. 

\textbf{Datasets and Evaluation} 
ASKTOACT utilizes simulated home environments with ambiguous instructions, where rewards are derived from privileged environment states, and is evaluated on object fetching tasks against GPT-4 and supervised MLLMs. Embodied-R trains on 5,000 embodied video samples processed with keyframe extraction for efficiency, with testing conducted across both in-distribution and out-of-distribution spatial reasoning tasks compared to OpenAI and Gemini models. The Embodied-Reasoner system generates 9,300 synthetic trajectories containing 64,000 images and 90,000 reasoning steps, supplemented by 809 human-validated test cases, and demonstrates superior performance in real-world object search tasks when benchmarked against OpenAI and Claude models, particularly in success rates and logical consistency metrics.

\section*{Preliminary Method}
blablabla
\section*{Expected Results}
blablabla

\printbibliography[title=References]
\end{document}